# Terraform Module: Ray on Kubernetes (EKS)

[![Terraform Validate](https://github.com/ambicuity/Terraform-Driven-Ray-on-Kubernetes-Platform/actions/workflows/terraform-ci.yml/badge.svg)](https://github.com/ambicuity/Terraform-Driven-Ray-on-Kubernetes-Platform/actions/workflows/terraform-ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-grade Terraform module for provisioning a robust, autoscaling Kubernetes (EKS) cluster optimized for Ray ML workloads. 

This module provides the necessary AWS infrastructure including VPC networking, EKS control plane, autoscaling CPU/GPU node groups, IAM Roles for Service Accounts (IRSA), and necessary security groups. It is designed to be highly configurable and ready to integrate with the [KubeRay Operator](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started.html).

## Features

- **EKS Cluster**: Provisions a fully functional EKS cluster.
- **Node Groups**: Supports separate, autoscaling CPU and GPU node groups.
- **GPU Taints & Tolerations**: GPU nodes are automatically tainted to prevent non-GPU workloads from consuming expensive resources.
- **Autoscaler Ready**: Configures IAM permissions and service accounts for the Kubernetes Cluster Autoscaler.
- **EBS CSI Driver Integration**: Configures IRSA permissions for the EBS CSI driver to allow Ray to provision persistent volumes.
- **Security best practices**: Configures private subnets for nodes, IRSA for pod-level permissions, and encrypts EBS volumes by default.

## Usage

Here is a minimal example of how to use this module:

```hcl
module "ray_eks_cluster" {
  source = "github.com/ambicuity/Terraform-Driven-Ray-on-Kubernetes-Platform"

  cluster_name = "my-ray-cluster"
  region       = "us-east-1"
  vpc_cidr     = "10.0.0.0/16"

  cpu_node_min_size     = 2
  cpu_node_max_size     = 10
  cpu_node_desired_size = 2

  enable_gpu_nodes      = true
  gpu_node_min_size     = 0
  gpu_node_max_size     = 5
  gpu_node_desired_size = 0
}
```

For a complete runnable example, see the [examples/complete](examples/complete) directory.

## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | >= 1.6.0 |
| <a name="requirement_aws"></a> [aws](#requirement\_aws) | >= 5.0 |
| <a name="requirement_kubernetes"></a> [kubernetes](#requirement\_kubernetes) | >= 2.0 |
| <a name="requirement_tls"></a> [tls](#requirement\_tls) | >= 4.0 |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| `cluster_name` | Name of the EKS cluster | `string` | `"ray-ml-cluster"` | no |
| `region` | AWS region | `string` | `"us-east-1"` | no |
| `vpc_cidr` | CIDR block for the VPC | `string` | `"10.0.0.0/16"` | no |
| `enable_gpu_nodes` | Whether to create a GPU node group | `bool` | `true` | no |
| `cpu_node_instance_types` | Instance types for CPU nodes | `list(string)` | `["m5.xlarge", "m5a.xlarge"]` | no |
| `cpu_node_min_size` | Minimum size of CPU node group | `number` | `2` | no |
| `cpu_node_max_size` | Maximum size of CPU node group | `number` | `10` | no |
| `gpu_node_instance_types` | Instance types for GPU nodes | `list(string)` | `["g4dn.xlarge"]` | no |
| `gpu_node_min_size` | Minimum size of GPU node group | `number` | `0` | no |
| `gpu_node_max_size` | Maximum size of GPU node group | `number` | `5` | no |

*(For a full list of inputs, see `variables.tf`)*

## Outputs

| Name | Description |
|------|-------------|
| `cluster_name` | EKS cluster name |
| `cluster_endpoint` | EKS cluster endpoint URL |
| `kubeconfig_command` | Command to configure kubectl |

*(For a full list of outputs, see `outputs.tf`)*

## Deploying Workloads

This module **only** provisions the AWS infrastructure. To deploy Ray itself, you should use the official [KubeRay Helm Chart](https://github.com/ray-project/kuberay/tree/master/helm-chart/kuberay-operator) and provide the cluster endpoint generated by this module.

An example Helm configuration (`values.yaml`) for a bursty Ray workload is provided in the `helm/ray/` directory of this repository for reference.

## License

MIT License. See [LICENSE](LICENSE) for details.
