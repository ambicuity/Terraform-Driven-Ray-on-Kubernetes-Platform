# Ray Cluster Configuration
# Optimized for bursty ML workloads with autoscaling

# Image configuration
image:
  repository: rayproject/ray
  tag: 2.9.0-py310
  pullPolicy: IfNotPresent

# Head node configuration
head:
  replicas: 1
  
  resources:
    requests:
      cpu: "2"
      memory: "8Gi"
    limits:
      cpu: "4"
      memory: "16Gi"
  
  # Persistent storage for head node
  volumeMounts:
    - name: ray-storage
      mountPath: /tmp/ray
  
  volumes:
    - name: ray-storage
      persistentVolumeClaim:
        claimName: ray-head-storage
  
  # Problem #1 Fix: Custom readinessProbe to ensure GCS is ready
  readinessProbe:
    exec:
      command: ["ray", "health-check"]
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3

  # Problem #1 Fix: Graceful termination to prevent 502s
  lifecycle:
    preStop:
      exec:
        command: ["/bin/sh", "-c", "ray stop --grace-period=30 && sleep 5"]
  
  terminationGracePeriodSeconds: 60
  
  # Service configuration
  service:
    type: ClusterIP
    ports:
      dashboard: 8265
      client: 10001
      redis: 6379
  
  # Topology Spread Constraints for High Availability
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          ray.io/node-type: head
  
  # Labels and annotations
  labels:
    ray.io/node-type: head
    app: ray
    component: head
  
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8265"
  
  # Environment variables
  env:
    - name: RAY_GRAFANA_HOST
      value: "http://grafana:3000"
    - name: RAY_PROMETHEUS_HOST
      value: "http://prometheus:9090"

# CPU Worker pool configuration
cpuWorkers:
  enabled: true
  replicas: 3
  minReplicas: 2
  maxReplicas: 10
  
  resources:
    requests:
      cpu: "2"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "8Gi"
      
  volumeMounts:
    - name: ray-tmp
      mountPath: /tmp/ray
      
  volumes:
    - name: ray-tmp
      emptyDir: {}
  
  # Node selector for CPU nodes
  nodeSelector:
    ray.io/resource-type: cpu
  
  # No tolerations needed (standard nodes)
  tolerations: []
  
  # Topology Spread Constraints for High Availability
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          ray.io/node-type: worker
          ray.io/resource-type: cpu
          
  labels:
    ray.io/node-type: worker
    ray.io/resource-type: cpu
    app: ray
    component: worker

  # Problem #2 Fix: Override injected GCS init container to avoid OOM
  initContainers:
    - name: wait-gcs-ready
      image: python:3.10-slim
      # Fallback to pure bash/python to avoid 180MB memory footprint of "ray health-check"
      command: ["/bin/sh", "-c"]
      args:
        - |
          echo "Waiting for GCS to become ready..."
          python -c "
          import socket, time, os
          host = os.environ.get('RAY_IP', 'ray-cluster-head-svc')
          port = 6379 
          timeout = 300
          start = time.time()
          while time.time() - start < timeout:
              try:
                  socket.create_connection((host, port), timeout=2)
                  print('GCS is reachable.')
                  exit(0)
              except Exception:
                  time.sleep(2)
          print('GCS not reachable after timeout.')
          exit(1)
          "
      resources:
        limits:
          cpu: "100m"
          memory: "512Mi"
        requests:
          cpu: "50m"
          memory: "128Mi"
  
  # Autoscaling configuration
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Percent
            value: 100
            periodSeconds: 30
          - type: Pods
            value: 2
            periodSeconds: 30

# GPU Worker pool configuration
gpuWorkers:
  enabled: true
  replicas: 0
  minReplicas: 0
  maxReplicas: 5
  
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
      
  volumeMounts:
    - name: ray-tmp
      mountPath: /tmp/ray
      
  volumes:
    - name: ray-tmp
      emptyDir: {}
  
  # Node selector for GPU nodes
  nodeSelector:
    ray.io/resource-type: gpu
    nvidia.com/gpu: "true"
  
  # Toleration for GPU taints
  tolerations:
    - key: nvidia.com/gpu
      operator: Equal
      value: "true"
      effect: NoSchedule
      
  # Topology Spread Constraints for High Availability
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          ray.io/node-type: worker
          ray.io/resource-type: gpu
  
  labels:
    ray.io/node-type: worker
    ray.io/resource-type: gpu
    app: ray
    component: gpu-worker

  # Problem #2 Fix: Override injected GCS init container to avoid OOM
  initContainers:
    - name: wait-gcs-ready
      image: python:3.10-slim
      # Fallback to pure bash/python to avoid 180MB memory footprint of "ray health-check"
      command: ["/bin/sh", "-c"]
      args:
        - |
          echo "Waiting for GCS to become ready..."
          python -c "
          import socket, time, os
          host = os.environ.get('RAY_IP', 'ray-cluster-head-svc')
          port = 6379 
          timeout = 300
          start = time.time()
          while time.time() - start < timeout:
              try:
                  socket.create_connection((host, port), timeout=2)
                  print('GCS is reachable.')
                  exit(0)
              except Exception:
                  time.sleep(2)
          print('GCS not reachable after timeout.')
          exit(1)
          "
      resources:
        limits:
          cpu: "100m"
          memory: "512Mi"
        requests:
          cpu: "50m"
          memory: "128Mi"
  
  # GPU autoscaling
  autoscaling:
    enabled: true
    minReplicas: 0
    maxReplicas: 5
    targetCPUUtilizationPercentage: 60
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 600
        policies:
          - type: Pods
            value: 1
            periodSeconds: 120
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Pods
            value: 1
            periodSeconds: 60

# Ray Dashboard configuration
dashboard:
  enabled: true
  
  ingress:
    enabled: false
    annotations: {}
    hosts: []
    tls: []

# Ray autoscaler configuration
autoscaler:
  enabled: true
  
  # Delegate autoscaling to the KubeRay operator
  enableInTreeAutoscaling: true
  
  # Idle timeout before scaling down (seconds)
  # Reduced to 30s for aggressive FinOps cost savings
  idleTimeoutSeconds: 30
  
  # Resource requests/limits for autoscaler
  resources:
    requests:
      cpu: "200m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

# Service Account
serviceAccount:
  create: true
  name: ray-worker
  annotations:
    eks.amazonaws.com/role-arn: ""  # Will be set by Terraform

# Security Context
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  runAsNonRoot: true

# Pod Security Policy
podSecurityPolicy:
  enabled: false

# Network Policy
networkPolicy:
  enabled: false

# Monitoring
monitoring:
  enabled: true
  
  # Prometheus ServiceMonitor
  serviceMonitor:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s

# Ray configuration
rayConfig:
  # Maximum number of workers
  maxWorkers: 15
  
  # Upscaling speed
  upscalingSpeed: 1.0
  
  # Idle timeout (matched to aggressive 30s autoscaler, 1 min is min here)
  idleTimeoutMinutes: 1
  
  # Resource requests
  headNodeType: "head-node"
  
  # Available node types
  availableNodeTypes:
    cpu-worker:
      minWorkers: 2
      maxWorkers: 10
      resources:
        cpu: 4
        memory: 8589934592  # 8Gi in bytes
    
    gpu-worker:
      minWorkers: 0
      maxWorkers: 5
      resources:
        cpu: 8
        memory: 34359738368  # 32Gi in bytes
        gpu: 1
      nodeLabels:
        nvidia.com/gpu: "true"

# Additional volumes for shared storage
additionalVolumes: []

# Additional volume mounts
additionalVolumeMounts: []
